%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \documentclass[11pt,oneside,a4paper, draft]{article}
\documentclass[12pt,oneside,a4paper]{article}
% \documentclass[11pt,oneside,a4paper]{letter}
\usepackage[a4paper, margin=1in]{geometry}   % MARGENS

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes básicos 
 \usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
\usepackage[english]{babel}
% \selectlanguage{english}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes de citações
\usepackage{natbib}	% Citações}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{mathtools, amsmath, amssymb, amsthm, latexsym}
\usepackage{lscape}				% Gira a página em 90 graus
%\usepackage{listings}			% Formatação para inserir códigos
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{xcolor}
\usepackage{ragged2e}           % formatação texto
\usepackage{bm}                 % bold symbols 
\usepackage[colorlinks, citecolor=blue,urlcolor=blue]{hyperref} % referencias dentro do texo (*QUEBRA MEMOIR*)
\usepackage{url}                % URL
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{subcaption}			% Faz subfiguras
\usepackage{textcase}			% MakeTextUppercase
%\usepackage{subfigure}         % subfigures
% \usepackage{setspace}         % Espaçamento
\usepackage{pdfpages}           % inclui páginas de pdfs (*FICHA CATALOGRAFICA*)
\usepackage[flushleft]{threeparttable} % notas nas tabelas
\usepackage{enumitem}
% \usepackage{titlesec}
\usepackage{titling}          % personalized other things
\usepackage{fancyhdr}         % personalized page style
\usepackage{lipsum}           % dummy text
% \usepackage[displaymath, pagewise]{lineno}           % show line numbers
\usepackage[]{lineno}           % show line numbers
% \linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{macros.tex}
% \pagestyle{headings}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METADATA
\title{Notes on Robust Hypothesis Testing}
\author{Paulo F. Naibert}
\date{\today}
% \thanks{} \\ 
%email\mailto{sth sthe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE PAGE
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{headings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TEXT
\section{Generalized Testing}
Based on \cite{lw2008-sr, lw2011-var}.

We observe $T$ pairs of returns, $(r_{1i}, r_{1n})', \dots, (r_{Ti}, r_{Tn})'$, with a bivariate return distribution:
% pairs as matrix
% $T \times 2$ matrix:
% 
% \begin{align}
% \begin{bmatrix}
% r_{1i} &  r_{1n} \\ \vdots \\ r_{Ti} & r_{Tn}
% \end{bmatrix}
% \end{align}

% NORMAL BIVARIATE
\begin{align*}
\mu=
\begin{bmatrix}
\mu_{i} \\ \mu_{n}
\end{bmatrix}
\quad
\Sigma=
\begin{bmatrix}
\sigma_{i}^2 & \sigma_{in}
\\
\sigma_{in} & \sigma_{n}^2
\end{bmatrix}.
\end{align*}

% sample counterparts
\textbf{NOTE:} Any quantity $x$ when denoted as $\hat{x}$ simply means its sample counterpart, e.g. $\hat{\mu}$ denotes the sample counterpart of $\mu$.

% Statistic
Define a primitive statistic $\tau$ as a function of data, define a ``derived'' statistic $\delta$ as a function of the primitive statistics, and finally, define $\Delta$ as the difference of derived statistics, then we have:
\begin{align*}
	\Delta = \delta_{i} - \delta_{n} = \delta(\tau(r_{ti})) - \delta(\tau(r_{tn}))
\end{align*}

We can also define $\Delta$ as a function of the $K \times 1$ vector of primitive statistics $v$:

\begin{align*}
	\Delta = f(v)
\end{align*}

Then we can define the $\nabla f(v)$ as the gradient of $f(v)$:

% gradient of f(v)
\begin{align*}
\nabla' f(v) &=
\left( 
\pdf{f(u)}{v_{1}},
\dots
\pdf{f(u)}{v_{n}},
\right)
\end{align*}

% 
Assume that:
\begin{align*}
	T^{1/2}(v - \hat{v}) \to^d N(0, \Psi),
\end{align*}
where $\Psi$ is an unknown symmetric PSD matrix.

If we apply a function on the vector $v$ of parameters, the Taylor expansion (Delta method) implies:
\begin{align*}
T^{1/2}[f(v) - f(\hat{v})] \to^d N \left( 0; \nabla'f(v) \Psi \nabla f(v)  \right).
\end{align*}

Well, we will use $f(\cdot)$ as defined earlier, and we denote $f(v) = \Delta$, so we have:
\begin{align*}
T^{1/2}(\Delta - \hat{\Delta}) 	\to^d N \left( 0; \nabla'f(v) \Psi \nabla f(v)  \right).
\end{align*}

Now, if a consistent estimator $\hat{\Psi}$ of $\Psi$ is available, then the standard error of the estimate $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:
% se(delta.hat)
\begin{align}
s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimating $\hat{\Psi}$}

Let's estimate $\hat{\Psi}$:

\begin{align*}
\Psi &= \lim_{T \to \infty} T^{-1} \sum_{s=1}^{T}\sum_{t=1}^{T} E[y_{s}y'_{t}],
\quad \text{with } \quad
y_{t}' = \left(  r_{ti} - \mu_{i}, r_{tn} - \mu_{n}, r^2_{ti} - \gamma_{i}, r^2_{tn} - \gamma_{n} \right)
\end{align*}

% psi fun
\begin{align*}
	\Psi &= \lim_{T \to \infty} \Psi_{t},
	\quad \text{ with } \quad
	\Psi = \sum_{j=-T+1}^{T-1} \Gamma_{T}(j),
	\qquad \text{where}
\end{align*}

% gamma fun
\begin{align*}
	\Gamma_{T}(j) &=
\begin{cases}
	T^{-1} \sum_{t=j+1}^{T}	E[y_{t}y'_{t-j}] & \text{for } j \geq 0
	\\
	T^{-1} \sum_{t=-j+1}^{T} E[y_{t+j}y'_{t}]& \text{for } j<0
\end{cases}
\end{align*}
\red{Are those $E(\cdot)$ necessary, since we are already taking means?}

% Psi hat
\begin{align*}
	\hat{\Psi} = \hat{\Psi}_{T} =
	\frac{T}{T-K}
	\sum_{j=-T+1}^{T-1}
	k\left( \frac{j}{S_{T}} \right) \hat{\Gamma}_{T}(j), && \text{where}
\end{align*}

$k(\cdot)$ kernel.
$S_{T}$ bandwidth.

% Gamma hat
\begin{align*}
	\hat{\Gamma}_{T}(j) &=
\begin{cases}
	T^{-1} \sum_{t=j+1}^{T}	\hat{y}_{t} \hat{y}'_{t-j} & \text{for } j \geq 0
	\\
	T^{-1} \sum_{t=-j+1}^{T} \hat{y}_{t+j} \hat{y}'_{t}& \text{for } j<0
\end{cases}
\end{align*}

\begin{quote}
{\footnotesize
The factor $T/(T-K)$ is a small sample degrees of freedom adjustment that is introduced to offset the effect of the estimation of the $K \times 1$ vector $v$ in the computation $\hat{\Gamma}_{T}(j)$, that is, the use of $\hat{y}_{t}$ rather than $y_{t}$.}
\end{quote}

A two-sided $p$-value for the Null hypothesis $H_{0}: \Delta=0$ is givenby:

\begin{align*}
	\hat{p} = 2 \Phi\left( \frac{|\hat{\Delta}|}{s(\hat{\Delta})} \right)
\end{align*}
where $\Psi$ denotes the cdf of the standard normal distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bootstrap Inference}

Circular Boostrap of \blue{Politis and Romano (1992)}, resampling now \textit{blocks of pairs} from the observed pairs $(r_{ti}, r_{tn})'$, $t=1, \dots, T$ \textit{with replacement}.
These block have a \textit{fixed} size $b\geq1$.
Standard error is computed as:
% standard error sd, se
\begin{align*}
se(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align*}
The estimator (\red{estimate?}) $\hat{\Psi}$ is obtained via kernel estimation.

Standard error $se(\hat{\Delta}^{*})$ is the ``natural'' standard error computed from the bootstrap data.
More specifically, letting $l=\floor{T/b}$, define:

\begin{align}
	y_{t}^{*} = \left(  r_{ti}^{*} - \hat{\mu}_{i}^{*}, r_{tn}^{*} - \hat{\mu}_{n}^{*}, r^2_{ti}^{*} - \hat{\gamma}_{i}^{*}, r^2_{tn}^{*} - \hat{\gamma}_{n}^{*} \right), 
	\quad t=1, \dots T
\end{align}

\begin{align}
	\zeta_{j}=b^{-1/2}\sum_{t=1}^{b} y_{t+(j-1)b}^{*},
	\quad t=1,\dots,l
\end{align}

and

\begin{align}
	\hat{\Psi}^{*}=l^{-1}\sum_{j=1}^{l} \zeta_{j} \zeta_{j}'.
\end{align}
With this more genral definition, of $\hat{\Psi}^{*}$, the bootstrap error is given by:

\begin{align*}
	se(\hat{\Delta}^{*}) =
	\sqrt{T^{-1} \nabla'f(\hat{v}^{*}) \hat{\Psi}^{*} \nabla f(\hat{v}^{*})}.
\end{align*}

\subsubsection{Direct Computation of the $p$-value}
\blue{Remark 3.2 on the original paper}

Denote the original studentized test statistic by:
\begin{align}
	d = \frac{|\hat{\Delta}|}{s(\hat{\Delta})}
\end{align}
and denote the \textit{centered} studentized test statistic computed from the $m$th bootstrap sample by: % $\tilde{d}^{*, m}$, $m=1, \dots, M$, that is:
\begin{align}
	\tilde{d}^{*, m} = \frac{|\hat{\Delta}^{*,m} - \hat{\Delta}|}{s(\hat{\Delta}^{*,m})}
\end{align}
where $M$ is the number of bootstrap resamples.
Then the $p$-value is computed as:
\begin{align}
	PV =\frac{\#\{ \tilde{d}^{*, m} \geq d\} + 1}{M+1}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Classical testing
\clearpage
\section{Sharpe Ratio Example}

Define the difference in Sharpe Ratios as function of the vector of primitive statistics $u = (\mu_{i}, \mu_{n}, \sigma^2_{i}, \sigma^2_{n})'$:
\begin{align}
\Delta = f(u) &= SR_{i} - SR_{n} 
= \frac{\mu_{i}}{\sqrt{\sigma^2_{i}}} - \frac{\mu_{n}}{\sqrt{\sigma^2_{n}}}.
\end{align}

Now, we need an estimator $\hat{\Psi}$ for $\Psi$, to find the standar error:
% se(delta.hat)
\begin{align}
s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{u}) \hat{\Psi} \nabla f(\hat{u})}.
\end{align}

The problem in \cite{jb81-sr} is that they uses a formula for $\hat{\Psi}$ that crucially relies on $iid$ return data for a bivariate normal distribution:
\begin{align*}
\Psi = 
\begin{bmatrix}
\sigma^2_{i} & \sigma_{in} & 0 & 0 \\
\sigma_{in} & \sigma^2_{n} & 0 & 0 \\
0 & 0 & 2\sigma^{4}_{i} & 2\sigma^{2}_{in}  \\
0 & 0 & 2\sigma^{2}_{in} & 2\sigma^{4}_{n}  \\
\end{bmatrix}
\end{align*}

Further information can be found in: \cite{lo02-sr, jb81-sr}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ledoit SR
\subsection{Ledoit and Wolf (2008)}

They correct \cite{jb81-sr} using robust estimates for $\Psi$.
They also use the bootstrap to find better $p$-values.

They work with the uncentered second moments, so we define 
\begin{align*}
	\gamma_{i} = E(r_{it}^{2}), \; \gamma_{n} = E(r_{nt}^{2}).
\end{align*}

Now, define $\Delta$ as a function of $v = (\mu_{i}, \mu_{n}, \gamma_{i}, \gamma_{n})'$, the vector of primitive statistics:

\begin{align}
\Delta = f(v) &= SR_{i} - SR_{n} = 
\frac{\mu_{i}}{\sqrt{\gamma_{i} - \mu^2_{i}}} - \frac{\mu_{n}}{\sqrt{\gamma_{n} - \mu^2_{n}}}.
\end{align}

The gradient of $f(v)$ is:
\begin{align*}
\nabla' f(v) &=
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}},
\pdf{f(v)}{\gamma_{i}},
\pdf{f(v)}{\gamma_{n}}
\right)
\\ &=
\left( 
\frac{ \gamma_{i}}{(\gamma_{i} - \mu_{i}^2)^{3/2}},
\frac{-\gamma_{n}}{(\gamma_{n} - \mu_{n}^2)^{3/2}}, 
\frac{-\mu_{i}}{2(\gamma_{i} - \mu_{i}^2)^{3/2}},  
\frac{ \mu_{n}}{2(\gamma_{n} - \mu_{n}^2)^{3/2}}
\right).
\end{align*}

And the standard error, $s(\hat{\Delta})$, is given by:
\begin{align*}
s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align*}
where $\hat{\Psi}$ is a consistent estimator for $\Psi$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% VARIANCE TESTING
\clearpage 

\section{Variance Testing}

\textbf{The classical $F$-test:}

% Classical Test
Define the reason of variances as:
\begin{align}
	\Theta=\sigma^2_{i}/\sigma^2_{n}.
\end{align}
What we want to test is if the reason of variances is different from one statistically significant.
\begin{align*}
	H_{0}: \,\Theta = 1
	\text{ vs. }
	H_{1}: \,\Theta \neq 1.
\end{align*}

Defining the test statistic as $F=\hat{\sigma}^2_{i}/\hat{\sigma}^2_{n}$.
$F_{\lambda, k_{1}, k_{2}}$ is the $\lambda$-quantile of $F_{k1, k2}$, or the $F$ distribution with $k_{1}$ and $k_{2}$ degrees of freedom.
The $F$-test rejects $H_{0}$ at a significance level $\alpha$ iff:
\begin{align*}
	F < F_{\alpha/2, T-1, T-1} 
	\text{ or, }
	F > F_{1-\alpha/2, T-1, T-1}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LW var
\subsection{Ledoit and Wolf (2011)}

First, we reformulate the problem with a log transformation: 
\begin{align}
\Delta = \log(\Theta) =
\log(\sigma_{i}^{2}) - \log(\sigma_{n}^{2})  =
\log(\sigma_{i}^{2}/\sigma_{n}^{2}).
\end{align}

\noindent
{\footnotesize
\textbf{OBS:} Log transformation, see Efron and Tibshirani 1993, sec 12.6.}
\vspace{1 em}

What we are going to test is whether the difference in log variances, $\Delta$, is zero or not.
In other terms:
\begin{align*}
	H_{0}: \Delta=0 \text{ vs. } H_{1}: \Delta \neq 0.
\end{align*}

Define the uncentered second moments as:
\begin{align*}
	\gamma_{i} = E(r_{it}^{2}), \; \gamma_{n} = E(r_{nt}^{2}),
\end{align*}

Then, we define $\Delta$ as a function of primitive statistics, $v = (\mu_{i}, \mu_{n}, \gamma_{i}, \gamma_{n})'$:
\begin{align*}
\Delta = f(v) &=
\log{(\gamma_{i} - \mu^2_{i})} - \log{(\gamma_{n} - \mu^2_{n})} \\ &=
\log\left( {\frac{\gamma_{i} - \mu^2_{i}}{\gamma_{n} - \mu^2_{n}}} \right).
\end{align*}

The gradient of the function $f(v)$ is:
\begin{align}
\nabla' f(v) &=
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}},
\pdf{f(v)}{\gamma_{i}},
\pdf{f(v)}{\gamma_{n}}
\right)
\\ &=
\left( 
\frac{-2 \mu_{i}}{\gamma_{i} - \mu_{i}^2},
\frac{2 \mu_{n}}{\gamma_{n} - \mu_{n}^2},
\frac{1}{\gamma_{i} - \mu_{i}^2},  
\frac{1}{\gamma_{n} - \mu_{n}^2}
\right).
\end{align}

And the standard error of $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:
\begin{align}
	s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align}
where $\hat{\Psi}$ is a consistent estimator for $\Psi$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MEAN TESTING
\clearpage
\section{Mean Testing}

\textbf{The classical $T$-test:}

% Differences
Define the difference of returns as $r_{td} = r_{ti} - r_{tn}$ with moments:
\begin{align*}
E(r_{td}) &= \mu_{d} = \mu_{i} - \mu_{n} 
	\\
V(r_{dt})&= E[(r_{ti} - r_{tn})^2] - [E(r_{ti} - r_{tn})]^2
\\
V(r_{dt})&= E(r^2_{ti}) + E(r^2_{tn}) - 2E(r_{ti}r_{tn}) - \mu_{i}^2 + \mu_{n}^2 - 2\mu_{i}\mu_{n}
\\
V(r_{dt}) &= V(r_{ti}) + V(r_{tn}) - 2Cov(r_{ti},r_{tn}) 
\\
\sigma^2_{d} &= \sigma^2_{i} + \sigma^2_{n} - 2\sigma_{in}
\end{align*}

% Classical Test
What we want to test is if the difference in means is different from zero statistically significant.
\begin{align*}
	H_{0}: \, \hat{\mu}_{d} = 0
	\text{ vs. }
	H_{1}: \, \hat{\mu}_{d} \neq 0.
\end{align*}

Define the test statistic as:
\begin{align}
\Theta &= T^{1/2} \frac{\hat{\mu}_{d}}{\hat{\sigma}_{d}} = 
 T^{1/2} \frac{\hat{\mu}_{i} - \hat{\mu}_{n}}{\hat{\sigma}^2_{i} + \hat{\sigma}^2_{n} - 2\hat{\sigma}_{in}}.
\end{align}
Further, let's define $t_{\lambda}(k)$ as the $\lambda$-quantile of $t(k)$, or the $t$ distribution with $k$ degrees of freedom.
The test rejects $H_{0}$ at a significance level $\alpha$ iff:
\begin{align*}
	|\Theta| > t_{1-\alpha/2}(T-1).
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFORMULATION
\section{Reformulation}
Based on \cite{lw2008-sr, lw2011-var}.

What we are going to test is whether the difference in mean, $\Delta$, is zero or not.
In other terms:
\begin{align*}
	H_{0}: \Delta=0 \text{ vs. } H_{1}: \Delta \neq 0.
\end{align*}

Define $\Delta$ as a function of $v = (\mu_{i}, \mu_{n})'$:
\begin{align*}
\Delta = f(v) = \mu_{i} - \mu_{n}.
\end{align*}

The gradient of the function $f(v)$ is:
\begin{align}
\nabla' f(v) =
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}}
\right) = \left( 1, -1 \right).
\end{align}

And the standard error of $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:

\begin{align*}
s(\hat{\Delta}) &= \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})} 
\\
\nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v}) &=
\begin{bmatrix} 1 & -1 \end{bmatrix}
\begin{bmatrix}
\hat{\psi}_{11} &  \hat{\psi}_{12}
\\            
\hat{\psi}_{21} &  \hat{\psi}_{22}
\end{bmatrix}
\begin{bmatrix} 1 \\  -1 \end{bmatrix}
\\
&= \hat{\psi}_{11} + \hat{\psi}_{22} -2\hat{\psi}_{12}
\end{align*}

If we use $\hat{\Psi}$ as the sample covariance matrix:
\begin{align*}
\nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})= \hat{\sigma}_{1}^2 + \hat{\sigma}_{22}^{2} -2\hat{\sigma}_{12}.
\end{align*}
And we have the same case as the classical $t$-test.
So we use HAC consistend methods to estimate $\hat{\Psi}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CEQ TESTING
\clearpage
\section{CEQ Testing}

Based on \cite{lw2008-sr, lw2011-var}.

Define $\Delta$ as a function of $v = (\mu_{i}, \mu_{n}, \gamma_{i}, \gamma_{n})'$:
\begin{align*}
\Delta = f(v) &= CEQ_{i} - CEQ_{n} 
\\ &= \mu_{i} - \frac{\theta}{2}(\gamma_{i} - \mu^2_{i}) -
\mu_{n} + \frac{\theta}{2}(\gamma_{n} - \mu^2_{n}).
\end{align*}

The gradient of the function $f(v)$ is:
\begin{align*}
\nabla' f(v) &=
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}}
\pdf{f(v)}{\gamma_{i}},
\pdf{f(v)}{\gamma_{n}}
\right) \\
&= \left( 1 + \theta \mu_{i}, -(1 + \theta \mu_{n}), -\frac{\theta}{2}, \frac{\theta}{2}  \right).
\end{align*}

And the standard error of $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:
\begin{align*}
	s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align*}
where $\hat{\Psi}$ is a consistent estimator for $\Psi$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Bibliografia
\clearpage
\bibliographystyle{authordate3} % bibliography style
\renewcommand\bibname{REFERENCES} 
\bibliography{../refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Bibliografia
\clearpage
\bibliographystyle{authordate3} % bibliography style
\renewcommand\bibname{REFERENCES} 
\bibliography{../refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
