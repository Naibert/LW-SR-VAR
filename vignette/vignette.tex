%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \documentclass[11pt,oneside,a4paper, draft]{article}
\documentclass[12pt,oneside,a4paper]{article}
% \documentclass[11pt,oneside,a4paper]{letter}
\usepackage[a4paper, margin=1in]{geometry}   % MARGENS

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes básicos 
 \usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
\usepackage[english]{babel}
% \selectlanguage{english}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes de citações
\usepackage{natbib}	% Citações}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{mathtools, amsmath, amssymb, amsthm, latexsym}
\usepackage{lscape}				% Gira a página em 90 graus
%\usepackage{listings}			% Formatação para inserir códigos
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{xcolor}
\usepackage{ragged2e}           % formatação texto
\usepackage{bm}                 % bold symbols 
\usepackage[colorlinks, citecolor=blue,urlcolor=blue]{hyperref} % referencias dentro do texo (*QUEBRA MEMOIR*)
\usepackage{url}                % URL
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{subcaption}			% Faz subfiguras
\usepackage{textcase}			% MakeTextUppercase
%\usepackage{subfigure}         % subfigures
% \usepackage{setspace}         % Espaçamento
\usepackage{pdfpages}           % inclui páginas de pdfs (*FICHA CATALOGRAFICA*)
\usepackage[flushleft]{threeparttable} % notas nas tabelas
\usepackage{enumitem}
% \usepackage{titlesec}
\usepackage{titling}          % personalized other things
\usepackage{fancyhdr}         % personalized page style
\usepackage{lipsum}           % dummy text
% \usepackage[displaymath, pagewise]{lineno}           % show line numbers
\usepackage[]{lineno}           % show line numbers
% \linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{macros.tex}
% \pagestyle{headings}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METADATA
\title{Notes on Ledoit and Wolf (2008, 2011)}
\author{Paulo F. Naibert}
\date{\today}
% \thanks{} \\ 
%email\mailto{sth sthe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE PAGE
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{headings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TEXT
\section{Generalized Testing}
Based on \cite{lw2008-sr, lw2011-var}.

We observe $T$ pairs of returns, $(r_{1i}, r_{1n})', \dots, (r_{Ti}, r_{Tn})'$, with a bivariate return distribution:
% pairs as matrix
% $T \times 2$ matrix:
% 
% \begin{align}
% \begin{bmatrix}
% r_{1i} &  r_{1n} \\ \vdots \\ r_{Ti} & r_{Tn}
% \end{bmatrix}
% \end{align}

% NORMAL BIVARIATE
\begin{align*}
\mu=
\begin{bmatrix}
\mu_{i} \\ \mu_{n}
\end{bmatrix}
\quad
\Sigma=
\begin{bmatrix}
\sigma_{i}^2 & \sigma_{in}
\\
\sigma_{in} & \sigma_{n}^2
\end{bmatrix}.
\end{align*}

% sample counterparts
\textbf{NOTE:} Any quantity $x$ when denoted as $\hat{x}$ simply means its sample counterpart, e.g. $\hat{\mu}$ denotes the sample counterpart of $\mu$.

% Statistic
Define a primitive statistic $\tau$ as a function of data, define a ``derived'' statistic $\delta$ as a function of the primitive statistics, and finally, define $\Delta$ as the difference of derived statistics, then we have:
\begin{align*}
	\Delta = \delta_{i} - \delta_{n} = \delta(\tau(r_{ti})) - \delta(\tau(r_{tn}))
\end{align*}

We can also define $\Delta$ as a function of the $K \times 1$ vector of primitive statistics $v$:

\begin{align*}
	\Delta = f(v)
\end{align*}

Then we can define the $\nabla f(v)$ as the gradient of $f(v)$:

% gradient of f(v)
\begin{align*}
\nabla' f(v) &=
\left( 
\pdf{f(u)}{v_{1}},
\dots
\pdf{f(u)}{v_{n}},
\right)
\end{align*}

% 
Assume that:
\begin{align*}
	T^{1/2}(v - \hat{v}) \to^d N(0, \Psi),
\end{align*}
where $\Psi$ is an unknown symmetric PSD matrix.

If we apply a function on the vector $v$ of parameters, the Taylor expansion (Delta method) implies:
\begin{align*}
T^{1/2}[f(v) - f(\hat{v})] \to^d N \left( 0; \nabla'f(v) \Psi \nabla f(v)  \right).
\end{align*}

Well, we will use $f(\cdot)$ as defined earlier, and we denote $f(v) = \Delta$, so we have:
\begin{align*}
T^{1/2}(\Delta - \hat{\Delta}) 	\to^d N \left( 0; \nabla'f(v) \Psi \nabla f(v)  \right).
\end{align*}

Now, if a consistent estimator $\hat{\Psi}$ of $\Psi$ is available, then the standard error of the estimate $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:
% se(delta.hat)
\begin{align}
s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimating $\hat{\Psi}$}

Let's estimate $\hat{\Psi}$:

\begin{align*}
\Psi &= \lim_{T \to \infty} T^{-1} \sum_{s=1}^{T}\sum_{t=1}^{T} E[y_{s}y'_{t}],
\quad \text{with } \quad
y_{t}' = \left(  r_{ti} - \mu_{i}, r_{tn} - \mu_{n}, r^2_{ti} - \gamma_{i}, r^2_{tn} - \gamma_{n} \right)
\end{align*}

% psi fun
\begin{align*}
	\Psi &= \lim_{T \to \infty} \Psi_{t},
	\quad \text{ with } \quad
	\Psi = \sum_{j=-T+1}^{T-1} \Gamma_{T}(j),
	\qquad \text{where}
\end{align*}

% gamma fun
\begin{align*}
	\Gamma_{T}(j) &=
\begin{cases}
	T^{-1} \sum_{t=j+1}^{T}	E[y_{t}y'_{t-j}] & \text{for } j \geq 0
	\\
	T^{-1} \sum_{t=-j+1}^{T} E[y_{t+j}y'_{t}]& \text{for } j<0
\end{cases}
\end{align*}
\red{Are those $E(\cdot)$ necessary, since we are already taking means?}

% Psi hat
\begin{align*}
	\hat{\Psi} = \hat{\Psi}_{T} =
	\frac{T}{T-K}
	\sum_{j=-T+1}^{T-1}
	k\left( \frac{j}{S_{T}} \right) \hat{\Gamma}_{T}(j), && \text{where}
\end{align*}

% Gamma hat
\begin{align*}
	\hat{\Gamma}_{T}(j) &=
\begin{cases}
	T^{-1} \sum_{t=j+1}^{T}	\hat{y}_{t} \hat{y}'_{t-j} & \text{for } j \geq 0
	\\
	T^{-1} \sum_{t=-j+1}^{T} \hat{y}_{t+j} \hat{y}'_{t}& \text{for } j<0
\end{cases}
\end{align*}

\begin{quote}
{\footnotesize
The factor $T/(T-K)$ is a small sample degrees of freedom adjustment that is introduced to offset the effect of the estimation of the $K \times 1$ vector $v$ in the computation $\hat{\Gamma}_{T}(j)$, that is, the use of $\hat{y}_{t}$ rather than $y_{t}$.}
\end{quote}

A two-sided $p$-value for the Null hypothesis $H_{0}: \Delta=0$ is givenby:

\begin{align*}
	\hat{p} = 2 \Phi\left( \frac{|\hat{\Delta}|}{s(\hat{\Delta})} \right)
\end{align*}
where $\Psi$ denotes the cdf of the standard normal distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bootstrap Inference}

Circular Boostrap of \blue{Politis and Romano (1992)}, resampling now \textit{blocks of pairs} from the observed pairs $(r_{ti}, r_{tn})'$, $t=1, \dots, T$ \textit{with replacement}.
These block have a \textit{fixed} size $b\geq1$.
Standard error is computed as:
% standard error sd, se
\begin{align*}
se(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align*}
The estimator (\red{estimate?}) $\hat{\Psi}$ is obtained via kernel estimation.

Standard error $se(\hat{\Delta}^{*})$ is the ``natural'' standard error computed from the bootstrap data.
More specifically, letting $l=\floor{T/b}$, define:

\begin{align}
	y_{t}^{*} = \left(  r_{ti}^{*} - \hat{\mu}_{i}^{*}, r_{tn}^{*} - \hat{\mu}_{n}^{*}, r^2_{ti}^{*} - \hat{\gamma}_{i}^{*}, r^2_{tn}^{*} - \hat{\gamma}_{n}^{*} \right), 
	\quad t=1, \dots T
\end{align}

\begin{align}
	\zeta_{j}=b^{-1/2}\sum_{t=1}^{b} y_{t+(j-1)b}^{*},
	\quad t=1,\dots,l
\end{align}

and

\begin{align}
	\hat{\Psi}^{*}=l^{-1}\sum_{j=1}^{l} \zeta_{j} \zeta_{j}'.
\end{align}
With this more genral definition, of $\hat{\Psi}^{*}$, the bootstrap error is given by:

\begin{align*}
	se(\hat{\Delta}^{*}) =
	\sqrt{T^{-1} \nabla'f(\hat{v}^{*}) \hat{\Psi}^{*} \nabla f(\hat{v}^{*})}.
\end{align*}

\subsubsection{Direct Computation of the $p$-value}
\blue{Remark 3.2 on the original paper}

Denote the original studentized test statistic by:
\begin{align}
	d = \frac{|\hat{\Delta}|}{s(\hat{\Delta})}
\end{align}
and denote the \textit{centered} studentized test statistic computed from the $m$th bootstrap sample by: % $\tilde{d}^{*, m}$, $m=1, \dots, M$, that is:
\begin{align}
	\tilde{d}^{*, m} = \frac{|\hat{\Delta}^{*,m} - \hat{\Delta}|}{s(\hat{\Delta}^{*,m})}
\end{align}
where $M$ is the number of bootstrap resamples.
Then the $p$-value is computed as:
\begin{align}
	PV =\frac{\#\{ \tilde{d}^{*, m} \geq d\} + 1}{M+1}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Classical testing
\clearpage
\section{Sharpe Ratio Example}

Define the difference in Sharpe Ratios as function of the vector $u = (\mu_{i}, \mu_{n}, \sigma^2_{i}, \sigma^2_{n})'$:
\begin{align}
\Delta = f(u) &= SR_{i} - SR_{n} 
= \frac{\mu_{i}}{\sqrt{\sigma^2_{i}}} - \frac{\mu_{n}}{\sqrt{\sigma^2_{n}}}.
\end{align}

Now, we need an estimator $\hat{\Psi}$ for $\Psi$, to find the standar error:
% se(delta.hat)
\begin{align}
s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{u}) \hat{\Psi} \nabla f(\hat{u})}.
\end{align}

The problem in \cite{jb81-sr} is that they uses a formula for $\hat{\Psi}$ that crucially relies on $iid$ return data for a bivariate normal distribution:
\begin{align*}
\Psi = 
\begin{bmatrix}
\sigma^2_{i} & \sigma_{in} & 0 & 0 \\
\sigma_{in} & \sigma^2_{n} & 0 & 0 \\
0 & 0 & 2\sigma^{4}_{i} & 2\sigma^{2}_{in}  \\
0 & 0 & 2\sigma^{2}_{in} & 2\sigma^{4}_{n}  \\
\end{bmatrix}
\end{align*}

Further information can be found in: \cite{lo02-sr, jb81-sr}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ledoit SR
\subsection{Ledoit and Wolf (2008)}

They correct \cite{jb81-sr} using robust estimates for $\Psi$.
They also use the bootstrap to find better $p$-values.

They work with the uncentered second moments, so we define 
\begin{align*}
	\gamma_{i} = E(r_{it}^{2}), \; \gamma_{n} = E(r_{nt}^{2}),
\end{align*}
then, our vector of primitive statistics is
\begin{align*}
v = (\mu_{i}, \mu_{n}, \gamma_{i}, \gamma_{n})'.
\end{align*}

Defining $\Delta$ as a function of $v$:
\begin{align}
\Delta = f(v) &= SR_{i} - SR_{n} = 
\frac{\mu_{i}}{\sqrt{\gamma_{i} - \mu^2_{i}}} - \frac{\mu_{n}}{\sqrt{\gamma_{n} - \mu^2_{n}}}.
\end{align}

The gradient of $f(v)$ is:
\begin{align*}
\nabla' f(v) &=
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}},
\pdf{f(v)}{\gamma_{i}},
\pdf{f(v)}{\gamma_{n}}
\right)
\\ &=
\left( 
\frac{ \gamma_{i}}{(\gamma_{i} - \mu_{i}^2)^{3/2}},
\frac{-\gamma_{n}}{(\gamma_{n} - \mu_{n}^2)^{3/2}}, 
\frac{-\mu_{i}}{2(\gamma_{i} - \mu_{i}^2)^{3/2}},  
\frac{ \mu_{n}}{2(\gamma_{n} - \mu_{n}^2)^{3/2}}
\right).
\end{align*}

Now the standar error, $s(\hat{\Delta})$, is given by:
\begin{align*}
s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align*}

with a robust estimator for $\Psi$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% VARIANCE TESTING
\clearpage \section{Variance Testing}
\cite{lw2011-var}

$T$ pairs: $(r_{1i}, r_{1n})', \dots, (r_{Ti}, r_{Tn})'$.

% pairs as matrix
% $T \times 2$ matrix:
% \begin{align*}
% \begin{bmatrix}
% r_{1i} &  r_{1n} \\ \vdots \\ r_{Ti} & r_{Tn}
% \end{bmatrix}
% \end{align*}

Bivariate return distribution:
\begin{align*}
\mu=
\begin{bmatrix}
\mu_{i} \\ \mu_{n}
\end{bmatrix}
\quad
\Sigma=
\begin{bmatrix}
\sigma_{i}^2 & \sigma_{ni}
\\
\sigma_{in} & \sigma_{n}^2
\end{bmatrix}.
\end{align*}

% sample counterparts
Any quantity $x$ when denoted as $\hat{x}$ simply means its sample counterpart, e.g. $\hat{\mu}$ denotes the sample counterpart of $\mu$.

% Classical Test
Define the reason of variances as:
\begin{align}
	\Theta=\sigma^2_{i}/\sigma^2_{n}.
\end{align}
What we want to test is if the reason of variances is different from one statistically significant.
\begin{align*}
	H_{0}: \,\Theta = 1
	\text{ vs. }
	H_{1}: \,\Theta \neq 1.
\end{align*}

\textbf{The classical $F$-test:}

Defining the test statistic as $F=\hat{\sigma}^2_{i}/\hat{\sigma}^2_{n}$.
$F_{\lambda, k_{1}, k_{2}}$ is the $\lambda$-quantile of $F_{k1, k2}$, or the $F$ distribution with $k_{1}$ and $k_{2}$ degrees of freedom.
The $F$-test rejects $H_{0}$ at a significance level $\alpha$ iff:
\begin{align*}
	F < F_{\alpha/2, T-1, T-1} 
	\text{ or, }
	F > F_{1-\alpha/2, T-1, T-1}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LW var
\subsection{Ledoit and Wolf (2011)}

First, we reformulate the problem with a log transformation: 
\begin{align}
\Delta = \log(\Theta) =
\log(\sigma_{i}^{2}) - \log(\sigma_{n}^{2})  =
\log(\sigma_{i}^{2}/\sigma_{n}^{2}).
\end{align}

\noindent
{\footnotesize
\textbf{OBS:} Log transformation, see Efron and Tibshirani 1993, sec 12.6.}
\vspace{1 em}

What we are going to test is whether the difference in log variances, $\Delta$, is zero or not.
In other terms:
\begin{align*}
	H_{0}: \Delta=0 \text{ vs. } H_{1}: \Delta \neq 0.
\end{align*}

Define the Uncentered Second Moments as:
\begin{align*}
	\gamma_{i} = E(r_{it}^{2}), \; \gamma_{n} = E(r_{nt}^{2}),
\end{align*}
Then, we define $\Delta$ as a function of $v = (\mu_{i}, \mu_{n}, \gamma_{i}, \gamma_{n})'$:
\begin{align*}
\Delta &= f(v),
\\
f(v) &=
\log{(\gamma_{i} - \mu^2_{i})} - \log{(\gamma_{n} - \mu^2_{n})} \\ &=
\log\left( {\frac{\gamma_{i} - \mu^2_{i}}{\gamma_{n} - \mu^2_{n}}} \right).
\end{align*}

The gradient of the function $f(v)$ is:
\begin{align}
\nabla' f(v) &=
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}},
\pdf{f(v)}{\gamma_{i}},
\pdf{f(v)}{\gamma_{n}}
\right)
\\ &=
\left( 
\frac{-2 \mu_{i}}{\gamma_{i} - \mu_{i}^2},
\frac{2 \mu_{n}}{\gamma_{n} - \mu_{n}^2},
\frac{1}{\gamma_{i} - \mu_{i}^2},  
\frac{1}{\gamma_{n} - \mu_{n}^2}
\right).
\end{align}

Now, assume that:
\begin{align}
	T^{1/2}(v - \hat{v}) \to^d N(0, \Psi),
\end{align}
where $\Psi$ is an unknown symmetric PSD matrix.

If we apply a function on the vector $v$ of parameters, the Taylor expansion (Delta method) implies:
\begin{align*}
	T^{1/2}[f(v) - f(\hat{v})] \to^d N \left( 0; \nabla'f(v) \Psi \nabla f(v)  \right).
\end{align*}

Well, we will use $f(\cdot)$ as defined earlier, and we denote $f(v) = \Delta$, so we have:
\begin{align*}
T^{1/2}(\Delta - \hat{\Delta}) 	\to^d N \left( 0; \nabla'f(v) \Psi \nabla f(v)  \right).
\end{align*}

Now, if a consistent estimator $\hat{\Psi}$ of $\Psi$ is available, then $se(\hat{\Delta})$ is given by:
\begin{align}
	se(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Bibliografia
\clearpage
\bibliographystyle{authordate3} % bibliography style
\renewcommand\bibname{REFERENCES} 
\bibliography{../refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
