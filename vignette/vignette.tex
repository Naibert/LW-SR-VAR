%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \documentclass[11pt,oneside,a4paper, draft]{article}
\documentclass[12pt,oneside,a4paper]{article}
% \documentclass[11pt,oneside,a4paper]{letter}
\usepackage[a4paper, margin=1in]{geometry}   % MARGENS

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes básicos 
 \usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
\usepackage[english]{babel}
% \selectlanguage{english}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes de citações
\usepackage{natbib}	% Citações}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{mathtools, amsmath, amssymb, amsthm, latexsym}
\usepackage{lscape}				% Gira a página em 90 graus
%\usepackage{listings}			% Formatação para inserir códigos
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{xcolor}
\usepackage{ragged2e}           % formatação texto
\usepackage{bm}                 % bold symbols 
\usepackage[colorlinks, citecolor=blue, urlcolor=blue, linkcolor=red]{hyperref} % referencias dentro do texo (*QUEBRA MEMOIR*)
\usepackage{url}                % URL
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{subcaption}			% Faz subfiguras
\usepackage{textcase}			% MakeTextUppercase
%\usepackage{subfigure}         % subfigures
% \usepackage{setspace}         % Espaçamento
\usepackage{pdfpages}           % inclui páginas de pdfs (*FICHA CATALOGRAFICA*)
\usepackage[flushleft]{threeparttable} % notas nas tabelas
\usepackage{enumitem}
% \usepackage{titlesec}
\usepackage{titling}          % personalized other things
\usepackage{fancyhdr}         % personalized page style
\usepackage{lipsum}           % dummy text
% \usepackage[displaymath, pagewise]{lineno}           % show line numbers
\usepackage[]{lineno}           % show line numbers
% \linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{macros.tex}
% \pagestyle{headings}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METADATA
\title{Notes on Robust Hypothesis Testing}
\author{Paulo F. Naibert}
\date{\today}
% \thanks{} \\ 
%email\mailto{sth sthe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE PAGE
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{headings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TEXT
\section{General Problem}
Based on \cite{lw2008, lw2011, lw2018}.

We observe $T$ pairs of returns, $(r_{1i}, r_{1n})', \dots, (r_{Ti}, r_{Tn})'$, with a bivariate return distribution over time:
% pairs as matrix
% $T \times 2$ matrix:
% 
% \begin{align}
% \begin{bmatrix}
% r_{1i} &  r_{1n} \\ \vdots \\ r_{Ti} & r_{Tn}
% \end{bmatrix}
% \end{align}

% NORMAL BIVARIATE
\begin{align*}
\mu=
\begin{bmatrix}
\mu_{i} \\ \mu_{n}
\end{bmatrix}
\quad
\Sigma=
\begin{bmatrix}
\sigma_{i}^2 & \sigma_{in}
\\
\sigma_{in} & \sigma_{n}^2
\end{bmatrix}.
\end{align*}

% assumptions
We do \textit{not} assume the distribution to be normal, nor do we assume that returns are independent over time.

% Statistic
The parameter of interest is:
\begin{align} \label{eq:delta}
	\Delta = \theta_{i} - \theta_{n},
\end{align}
where $\theta$ is a given performance measure.
Hence, $\theta_{i}$ is the performance measure for the strategy $i$, and $\theta_{n}$ is the performance measure for the strategy $n$.

We are interested in testing:
\begin{align*} \label{eq:hyp}
	H_{0}: \Delta = 0 \quad \text{vs} \quad	H_{1}: \Delta \neq 0  
\end{align*}

We consider the class of performance measure \textit{$\theta$ that can be expressed as a smooth function of a finite number of population moments}.
In particular, for $j=i,n$ let

\begin{align*}
	v_{j}^{(k)} := E(r_{j}^k)
\end{align*}

\noindent
denote the (uncentered) $k$th population moment of the returns of strategy $j$.
Then, for $j=i,n$ we assume that $\theta_{j}$ can be expressed as

\begin{align*}
	\theta_{j} = h( v_{j}^{(1)}, \dots, v_{j}^{(K)}), 
\end{align*}

\noindent
where $K \geq1$ is an integer and $h: \R^{K} \to \R$ is a smooth function (in the sense of being one time continuously differentiable).

For $j=i,n$, let $v_{j}'= (v_{j}^{(1)}, \dots, v_{j}^{(K)})$.
Furthermore, let $v'= (v_{i}', v_{n}')$.
Then the parameter of interest $\Delta$ in \eqref{eq:delta} can be written as a function of the population moments:

\begin{align*}
	\Delta := f(v) = f(v_{i} , v_{n}) = h(v_{i}) - h(v_{n}) = \theta_{i} - \theta_{n}, 
\end{align*}

so that $f: \R^{2K} \to \R$ is also a smooth function, defined as:

\begin{align}\label{eq:delta:fn}
	f(v_{i}^{(1)}, \dots, v_{i}^{(K)}, v_{n}^{(1)}, \dots, v_{n}^{(K)})  =
	h(v_{i}^{(1)}, \dots, v_{i}^{(K)})  -
	h(v_{n}^{(1)}, \dots, v_{n}^{(K)}) =
	\theta_{i} - \theta_{n}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sample Estimates}

For $j=i,n$, denote the (uncentered) $k$th sample moment of the observed returns by:
\begin{align*}
	\hat{v}_{j}^{(k)} := T^{-1} \sum_{t=1}^T r_{tj}^{k}.
\end{align*}

Then the estimator of the parameter of interest, $\Delta$, is given by:
\begin{align}
	\hat{\Delta} := \hat{\theta}_{i} - \hat{\theta}_{n}
\end{align}
where
\begin{align}
	\hat{\theta}_{j} := h(\hat{v}_{j}^{(1)}, \dots, \hat{v}_{j}^{(K)} ).
\end{align}

For $j=i,n$, let $\hat{v}_{j}' := (\hat{v}_{j}^{(1)}, \dots, \hat{v}_{j}^{(K)} )$.
Furthermore, let $\hat{v}' :=( \hat{v}_{i}', \hat{v}_{n}' )$.
Then the estimator of $\Delta$ can also be expressed as:
\begin{align*}
	\hat{\Delta} := f(\hat{v})
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solutions}
We assume that:
\begin{align}\label{eq:vhat:dist}
	T^{1/2}(\hat{v} - v) \to^d N(0, \Psi),
\end{align}
where $\Psi$ is an unknown symmetric PD matrix of dimension $2K \times 2K$.
This relation holds under mild regularity conditions.
For various sets of sufficient conditions in the TS case, see \blue{White (2001)}, for example.

If we apply a function on the vector $v$ of parameters, the Taylor expansion (Delta method) implies:
\begin{align*}
T^{1/2}[f(\hat{v}) - f(v)] \to^d N \left( 0; \nabla'f(v) \Psi \nabla f(v)  \right).
\end{align*}

Well, we will use $f(\cdot)$ as defined earlier, and we denote $f(v) = \Delta$, so we have:
\begin{align}\label{eq:dhat:dist}
	T^{1/2}(\hat{\Delta} - \Delta) 	\to^d N \left( 0; \nabla'f(v) \Psi \nabla f(v)  \right).
\end{align}
where the $2K \times 1$ vector-valued function $\nabla f(\cdot)$ is the gradient of $f(\cdot)$. 

Therefore, if a consistent estimator $\hat{\Psi}$ of $\Psi$ is available, then an asymptotic standard error of for $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:
% se(delta.hat)
\begin{align}\label{eq:sedelta}
s(\hat{\Delta}) := \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align}

Given  the formula \eqref{eq:delta:fn} for $f(\cdot)$, it hold that:

Then we can define the $\nabla f(v)$ as the gradient of $f(v)$:
% gradient of f(v)
\begin{align*}
\nabla' f(v) &=
\left( 	\nabla'h(v_{i}),  - \nabla'h(v_{n}) \right) \\ &=
\left( 
\pdf{f(v_{i})}{v_{i}^{(1)}},
\dots,
\pdf{f(v_{i})}{v_{i}^{(K)}},
-\pdf{f(v_{n})}{v_{n}^{(1)}},
\dots,
-\pdf{f(v_{n})}{v_{n}^{(K)}}
\right).
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textit{Note 1:} In the codes we use:
\begin{align*}
\nabla' f(v) &=
\left( 
\pdf{f(v_{i})}{v_{i}^{(1)}},
-\pdf{f(v_{n})}{v_{n}^{(1)}},
\dots ,
\pdf{f(v_{i})}{v_{i}^{(K)}},
-\pdf{f(v_{n})}{v_{n}^{(K)}}
\right).
\blacksquare
\end{align*}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimating $\hat{\Psi}$}

Let's estimate $\hat{\Psi}$:
\begin{align*}
\Psi = \lim_{T \to \infty} T^{-1} \sum_{s=1}^{T}\sum_{t=1}^{T} E[y_{s}y'_{t}],
\end{align*}
where
\begin{align*}
y_{t}' = 
\left(
r_{ti} - v_{i}^{(1)}, \dots , r_{ti}^K - v_{i}^{(K)},
r_{tn} - v_{n}^{(1)}, \dots , r_{tn}^K - v_{n}^{(K)} 
\right)
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textit{Note 2:} In the codes we use:
\begin{align*}
y_{t}' = 
\left(
r_{ti} - v_{i}^{(1)}, r_{tn} - v_{n}^{(1)}, \dots,
r_{ti}^K - v_{i}^{(K)}, r_{tn}^K - v_{n}^{(K)} 
\right).
\blacksquare
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% psi fun
\begin{align*}
	\Psi &= \lim_{T \to \infty} \Psi_{T},
	\quad \text{ with } \quad
	\Psi_{T} = \sum_{j=-T+1}^{T-1} \Gamma_{T}(j),
	\qquad \text{where}
\end{align*}

% gamma fun
\begin{align*}
\Gamma_{T}(j) &=
\begin{cases}
T^{-1} \sum_{t=j+1}^{T}	E[y_{t}y'_{t-j}] & \text{for } j \geq 0
\\
T^{-1} \sum_{t=-j+1}^{T} E[y_{t+j}y'_{t}]& \text{for } j<0
\end{cases}
\end{align*}
\red{Are those $E(\cdot)$ necessary, since we are already taking means?}

$k(\cdot)$ kernel function.
$S_{T}$ bandwidth.

% Psi hat
\begin{align*}
	\hat{\Psi} := \hat{\Psi}_{T} =
	\frac{T}{T-2K}
	\sum_{j=-T+1}^{T-1}
	k\left( \frac{j}{S_{T}} \right) \hat{\Gamma}_{T}(j), && \text{where}
\end{align*}

% Gamma hat
\begin{align*}
	\hat{\Gamma}_{T}(j) &=
\begin{cases}
	T^{-1} \sum_{t=j+1}^{T}	\hat{y}_{t} \hat{y}'_{t-j} & \text{for } j \geq 0
	\\
	T^{-1} \sum_{t=-j+1}^{T} \hat{y}_{t+j} \hat{y}'_{t}& \text{for } j<0
\end{cases}
\end{align*}
where
\begin{align*}
	\hat{y}_{t}' = 
\left(
	r_{ti} - \hat{v}_{i}^{(1)}, \dots , r_{ti}^K - \hat{v}_{i}^{(K)},
	r_{tn} - \hat{v}_{n}^{(1)}, \dots , r_{tn}^K - \hat{v}_{n}^{(K)} 
\right)
\end{align*}

The factor $T/(T-2K)$ is a small sample degrees of freedom adjustment that is introduced to offset the effect of the estimation of the $K \times 1$ vector $v$ in the computation $\hat{\Gamma}_{T}(j)$, that is, the use of $\hat{y}_{t}$ rather than $y_{t}$.

[\dots]

A two-sided $p$-value for the Null hypothesis $H_{0}: \Delta=0$ is given by:
\begin{align*}
	\hat{p} = 2 \Phi\left( \frac{|\hat{\Delta}|}{s(\hat{\Delta})} \right)
\end{align*}
where $\Psi$ denotes the cdf of the standard normal distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bootstrap Inference}

The two-sided distribution function of the studentized statistic is approximated via the bootstrap as follows:
\begin{align}
L\left(
\frac{| \hat{\Delta} - \Delta |}{s(\hat{\Delta})}
\right) 
\approx
L\left(
\frac{| \hat{\Delta}^{*} - \hat{\Delta} |}{s(\hat{\Delta}^{*})}
\right) 
\end{align}
where $\Delta$ is the true difference in the performance measure, 
$\hat{\Delta}$ is the estimated difference from the original data,
$\hat{\Delta}^{*}$ is the estimated difference from the bootstrap data.
$L(X)$ is the distribution function of random variable $X$.

We use the Circular Boostrap of \cite{pr92}, resampling now \textit{blocks of pairs} from the observed pairs $(r_{ti}, r_{tn})'$, $t=1, \dots, T$ \textit{with replacement}.
These block have a \textit{fixed} size $b\geq1$.
Standard error is computed as:
% standard error sd, se
\begin{align*}
se(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align*}
The estimator (\red{estimate?}) $\hat{\Psi}$ is obtained via kernel estimation.

Standard error $se(\hat{\Delta}^{*})$ is the ``natural'' standard error computed from the bootstrap data.
More specifically, letting $l=\floor{T/b}$, define:

\begin{align}
	y_{t}^{*} = \left(  r_{ti}^{*} - \hat{\mu}_{i}^{*}, r_{tn}^{*} - \hat{\mu}_{n}^{*}, r^2_{ti}^{*} - \hat{\gamma}_{i}^{*}, r^2_{tn}^{*} - \hat{\gamma}_{n}^{*} \right), 
	\quad t=1, \dots T
\end{align}

\begin{align}
	\zeta_{j}=b^{-1/2}\sum_{t=1}^{b} y_{t+(j-1)b}^{*},
	\quad t=1,\dots,l
\end{align}
and
\begin{align}
	\hat{\Psi}^{*}=l^{-1}\sum_{j=1}^{l} \zeta_{j} \zeta_{j}'.
\end{align}
With this more genral definition, of $\hat{\Psi}^{*}$, the bootstrap error is given by:
\begin{align*}
	se(\hat{\Delta}^{*}) =
	\sqrt{T^{-1} \nabla'f(\hat{v}^{*}) \hat{\Psi}^{*} \nabla f(\hat{v}^{*})}.
\end{align*}

\subsubsection{Direct Computation of the $p$-value}
\textit{Remark 3.2} of \cite{lw2008} or \textit{Remark 3.1} of \cite{lw2018}.

Denote the original studentized test statistic by:
\begin{align}
	d = \frac{|\hat{\Delta}|}{s(\hat{\Delta})}
\end{align}
and denote the \textit{centered} studentized test statistic computed from the $m$th bootstrap sample by: % $\tilde{d}^{*, m}$, $m=1, \dots, M$, that is:
\begin{align}
	\tilde{d}^{*, m} = \frac{|\hat{\Delta}^{*,m} - \hat{\Delta}|}{s(\hat{\Delta}^{*,m})}
\end{align}
where $M$ is the number of bootstrap resamples.
Then the $p$-value is computed as:
\begin{align}
	\hat{p}^{*} =\frac{\#\{ \tilde{d}^{*, m} \geq d\} + 1}{M+1}. 
	\quad \blacksquare
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Classical testing
\clearpage
\section{Sharpe Ratio Example}
\cite{lw2008}

Define the difference in Sharpe Ratios as function of the vector of primitive statistics $u = (\mu_{i}, \mu_{n}, \sigma^2_{i}, \sigma^2_{n})'$:
\begin{align}
\Delta = f(u) &= SR_{i} - SR_{n} 
= \frac{\mu_{i}}{\sqrt{\sigma^2_{i}}} - \frac{\mu_{n}}{\sqrt{\sigma^2_{n}}}.
\end{align}

Now, we need an estimator $\hat{\Psi}$ for $\Psi$, to find the standar error:
% se(delta.hat)
\begin{align}
s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{u}) \hat{\Psi} \nabla f(\hat{u})}.
\end{align}

The problem in \cite{jk81} is that they uses a formula for $\hat{\Psi}$ that crucially relies on $iid$ return data for a bivariate normal distribution:
\begin{align*}
\Psi = 
\begin{bmatrix}
\sigma^2_{i} & \sigma_{in} & 0 & 0 \\
\sigma_{in} & \sigma^2_{n} & 0 & 0 \\
0 & 0 & 2\sigma^{4}_{i} & 2\sigma^{2}_{in}  \\
0 & 0 & 2\sigma^{2}_{in} & 2\sigma^{4}_{n}  \\
\end{bmatrix}
\end{align*}

Further information can be found in: \cite{lo02, jk81}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ledoit SR
\subsection{Ledoit and Wolf (2008)}

They correct \cite{jk81} using robust estimates for $\Psi$.
They also use the bootstrap to find better $p$-values.

Define $\gamma_{i}$ as the uncentered second moment of $r_{ti}$.
We can define $\Delta$ as a function of $v = (\mu_{i}, \mu_{n}, \gamma_{i}, \gamma_{n})'$, the vector of the two first moments.
\begin{align}
\Delta = f(v) &= SR_{i} - SR_{n} = 
\frac{\mu_{i}}{\sqrt{\gamma_{i} - \mu^2_{i}}} - \frac{\mu_{n}}{\sqrt{\gamma_{n} - \mu^2_{n}}}.
\end{align}

The gradient of $f(v)$ is:
\begin{align*}
\nabla' f(v) &=
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}},
\pdf{f(v)}{\gamma_{i}},
\pdf{f(v)}{\gamma_{n}}
\right)
\\ &=
\left( 
\frac{ \gamma_{i}}{(\gamma_{i} - \mu_{i}^2)^{3/2}},
\frac{-\gamma_{n}}{(\gamma_{n} - \mu_{n}^2)^{3/2}}, 
\frac{-\mu_{i}}{2(\gamma_{i} - \mu_{i}^2)^{3/2}},  
\frac{ \mu_{n}}{2(\gamma_{n} - \mu_{n}^2)^{3/2}}
\right).
\end{align*}

And the standard error, $s(\hat{\Delta})$, is given by:
\begin{align*}
s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align*}
where $\hat{\Psi}$ is a consistent estimator for $\Psi$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% VARIANCE TESTING
\clearpage 
\section{Variance Testing}
\cite{lw2011}

\textbf{The classical $F$-test}, tests the reason of variances.
So, we define the reason of variances as:
\begin{align}
	\Theta=\sigma^2_{i}/\sigma^2_{n}.
\end{align}
What we want to test is if the reason of variances is different from one statistically significant.
\begin{align*}
	H_{0}: \,\Theta = 1
	\text{ vs. }
	H_{1}: \,\Theta \neq 1.
\end{align*}

Defining the test statistic as $F=\hat{\sigma}^2_{i}/\hat{\sigma}^2_{n}$.
$F_{\lambda, k_{1}, k_{2}}$ is the $\lambda$-quantile of $F_{k1, k2}$, or the $F$ distribution with $k_{1}$ and $k_{2}$ degrees of freedom.
The $F$-test rejects $H_{0}$ at a significance level $\alpha$ iff:
\begin{align*}
	F < F_{\alpha/2, T-1, T-1} 
	\text{ or, }
	F > F_{1-\alpha/2, T-1, T-1}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LW var
\subsection{Ledoit and Wolf (2011)}

They reformulate the problem with a log transformation: 
\begin{align}
\Delta = \log(\Theta) =
\log(\sigma_{i}^{2}) - \log(\sigma_{n}^{2})  =
\log(\sigma_{i}^{2}/\sigma_{n}^{2}).
\end{align}

\noindent
{\footnotesize
\textbf{OBS:} Log transformation, see Efron and Tibshirani 1993, sec 12.6.}
\vspace{1 em}

Defining $\Delta$ as a function of the first two uncentered moments, $v = (\mu_{i}, \mu_{n}, \gamma_{i}, \gamma_{n})'$:
\begin{align*}
\Delta = f(v) &=
\log{(\gamma_{i} - \mu^2_{i})} - \log{(\gamma_{n} - \mu^2_{n})} \\ &=
\log\left( {\frac{\gamma_{i} - \mu^2_{i}}{\gamma_{n} - \mu^2_{n}}} \right).
\end{align*}

The gradient of the function $f(v)$ is:
\begin{align}
\nabla' f(v) &=
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}},
\pdf{f(v)}{\gamma_{i}},
\pdf{f(v)}{\gamma_{n}}
\right)
\\ &=
\left( 
\frac{-2 \mu_{i}}{\gamma_{i} - \mu_{i}^2},
\frac{2 \mu_{n}}{\gamma_{n} - \mu_{n}^2},
\frac{1}{\gamma_{i} - \mu_{i}^2},  
\frac{1}{\gamma_{n} - \mu_{n}^2}
\right).
\end{align}

And the standard error of $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:
\begin{align}
	s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align}
where $\hat{\Psi}$ is a consistent estimator for $\Psi$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MEAN TESTING
\clearpage
\section{Mean Testing}

\textbf{The classical $T$-test} 
verifies if the difference in means is statistically significant different from zero.
So, we define the difference of returns as $r_{td} = r_{ti} - r_{tn}$ with moments:
\begin{align*}
E(r_{td}) &= \mu_{d} = \mu_{i} - \mu_{n} 
	\\
V(r_{dt})&= E[(r_{ti} - r_{tn})^2] - [E(r_{ti} - r_{tn})]^2
\\
V(r_{dt})&= E(r^2_{ti}) + E(r^2_{tn}) - 2E(r_{ti}r_{tn}) - \mu_{i}^2 + \mu_{n}^2 - 2\mu_{i}\mu_{n}
\\
V(r_{dt}) &= V(r_{ti}) + V(r_{tn}) - 2Cov(r_{ti},r_{tn}) 
\\
\sigma^2_{d} &= \sigma^2_{i} + \sigma^2_{n} - 2\sigma_{in}
\end{align*}

Define the test statistic as:
\begin{align}
\hat{\Delta} &= T^{1/2} \frac{\hat{\mu}_{d}}{\hat{\sigma}_{d}} = 
 T^{1/2} \frac{\hat{\mu}_{i} - \hat{\mu}_{n}}{\hat{\sigma}^2_{i} + \hat{\sigma}^2_{n} - 2\hat{\sigma}_{in}}.
\end{align}
Further, let's define $t_{\lambda}(k)$ as the $\lambda$-quantile of $t(k)$, or the $t$ distribution with $k$ degrees of freedom.
The test rejects $H_{0}$ at a significance level $\alpha$ iff:
\begin{align*}
	|\hat{\Delta}| > t_{1-\alpha/2}(T-1).
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFORMULATION
\subsection{Reformulation}
Based on \cite{lw2018}.

Define $\Delta$ as a function of $v = (\mu_{i}, \mu_{n})'$:
\begin{align*}
\Delta = f(v) = \mu_{i} - \mu_{n}.
\end{align*}

The gradient of the function $f(v)$ is:
\begin{align}
\nabla' f(v) =
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}}
\right) = \left( 1, -1 \right).
\end{align}

And the standard error of $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:

\begin{align*}
s(\hat{\Delta}) &= \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})} 
\\
\nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v}) &=
\begin{bmatrix} 1 & -1 \end{bmatrix}
\begin{bmatrix}
\hat{\psi}_{11} &  \hat{\psi}_{12}
\\            
\hat{\psi}_{21} &  \hat{\psi}_{22}
\end{bmatrix}
\begin{bmatrix} 1 \\  -1 \end{bmatrix}
\\
&= \hat{\psi}_{11} + \hat{\psi}_{22} -2\hat{\psi}_{12}
\end{align*}

If we use $\hat{\Psi}$ as the sample covariance matrix:
\begin{align*}
\nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})= \hat{\sigma}_{1}^2 + \hat{\sigma}_{22}^{2} -2\hat{\sigma}_{12}.
\end{align*}
And we have the same case as the classical $t$-test.
So we use HAC consistend methods to estimate $\hat{\Psi}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CEQ TESTING
\clearpage
\section{CEQ Testing}

Based on \cite{lw2008, lw2011}.

Define $\Delta$ as a function of $v = (\mu_{i}, \mu_{n}, \gamma_{i}, \gamma_{n})'$:
\begin{align*}
\Delta = f(v) &= CEQ_{i} - CEQ_{n} 
\\ &= \mu_{i} - \frac{\theta}{2}(\gamma_{i} - \mu^2_{i}) -
\mu_{n} + \frac{\theta}{2}(\gamma_{n} - \mu^2_{n}).
\end{align*}

The gradient of the function $f(v)$ is:
\begin{align*}
\nabla' f(v) &=
\left( 
\pdf{f(v)}{\mu_{i}},
\pdf{f(v)}{\mu_{n}}
\pdf{f(v)}{\gamma_{i}},
\pdf{f(v)}{\gamma_{n}}
\right) \\
&= \left( 1 + \theta \mu_{i}, -(1 + \theta \mu_{n}), -\frac{\theta}{2}, \frac{\theta}{2}  \right).
\end{align*}

And the standard error of $\hat{\Delta}$, $s(\hat{\Delta})$, is given by:
\begin{align*}
	s(\hat{\Delta}) = \sqrt{T^{-1} \nabla'f(\hat{v}) \hat{\Psi} \nabla f(\hat{v})}.
\end{align*}
where $\hat{\Psi}$ is a consistent estimator for $\Psi$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Bibliografia
\clearpage
\bibliographystyle{authordate3} % bibliography style
\renewcommand\bibname{REFERENCES} 
\bibliography{../refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Bibliografia
\clearpage
\bibliographystyle{authordate3} % bibliography style
\renewcommand\bibname{REFERENCES} 
\bibliography{./refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
